{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "816f606b-2c91-4866-988b-66e5fd1408b6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Refinement Systems\n",
    "In this notebook we are going to experiment with a new camera I am trying out xDDDDD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffa049e-110e-4d15-b23c-925bbb25b874",
   "metadata": {},
   "source": [
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1615cc6-7057-4955-8deb-6ef84b4ba56c",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "61ca3305-96ca-400a-92c6-4cffe46060a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import requests\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "05428687-9980-42b1-85c3-7df1633adfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "op_types = ['add', 'remove', 'replace']\n",
    "op_colors = {'add': px.colors.qualitative.Plotly[0], 'remove': px.colors.qualitative.Plotly[1], 'replace': px.colors.qualitative.Plotly[2]}\n",
    "pio.templates.default = \"plotly_white\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203606ba-332a-4975-b853-209dec1c7db8",
   "metadata": {},
   "source": [
    "### Loading top classes data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "c8faf8ff-5531-460e-be27-90274a73de82",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = os.path.join('..', 'data')\n",
    "\n",
    "NOTEBOOK1_OUTPUT_DIR = os.path.join('output', '1_data_fetching')\n",
    "CLASSES_FILE = os.path.join(NOTEBOOK1_OUTPUT_DIR, 'top_classes.pkl')\n",
    "\n",
    "OUTPUT_DIR = os.path.join('output', '3_edit_history_systems')\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "WIKIDATA_BASE = \"https://www.wikidata.org/w/api.php\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6389891f-788d-4dbb-ae9f-90f975c7b83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class KGEntity:\n",
    "    qid: str\n",
    "    pagerank_score: float\n",
    "\n",
    "@dataclass\n",
    "class KGClass:\n",
    "    name: str\n",
    "    qid: str\n",
    "    classrank_score: float\n",
    "    instances: List[KGEntity]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "c249d31e-f5e2-454a-9cfc-995e780c41c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CLASSES_FILE, 'rb') as f:\n",
    "    top_classes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9394a582-0241-47e4-8980-57aa628d217c",
   "metadata": {},
   "source": [
    "### Connecting to the database\n",
    "We will now make a connection to the Mongo database where this data is stored. If everything was set up from the docker-compose file, this connection will be configured automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b1f2ae7-d83b-4d31-8e32-bca9288dc7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "\n",
    "def env_or_callback(env_name, callback, *args):\n",
    "    return os.getenv(env_name) if os.getenv(env_name) is not None else callback(*args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "583a8022-a691-4895-90fe-60d20ae545aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import pymongo\n",
    "\n",
    "MONGO_USERNAME = env_or_callback(\"MONGO_USERNAME\", input, \"Mongo username: \")\n",
    "MONGO_PASSWORD = env_or_callback(\"MONGO_PASSWORD\", getpass.getpass, \"Mongo password: \")\n",
    "MONGO_URL = env_or_callback(\"MONGO_URL\", input, \"Mongo url: \")\n",
    "MONGO_DATABASE = \"wd_diff\"\n",
    "\n",
    "def get_database():\n",
    "    # Provide the mongodb atlas url to connect python to mongodb using pymongo\n",
    "    CONNECTION_STRING = f\"mongodb://{MONGO_USERNAME}:{MONGO_PASSWORD}@127.0.0.1:27017/{MONGO_DATABASE}\"\n",
    "\n",
    "    # Create a connection using MongoClient. You can import MongoClient or use pymongo.MongoClient\n",
    "    client = MongoClient(CONNECTION_STRING)\n",
    "\n",
    "    # Create the database for our example (we will use the same database throughout the tutorial\n",
    "    return client[MONGO_DATABASE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48b95858-39b1-460c-9eca-c828cd314c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = get_database()\n",
    "wd_entities = db.wd_entities\n",
    "wd_revisions = db.wd_revisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0a914eb1-9f36-4cac-a1ba-9b9fa3ed3bb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PURE_PROPERTY_EDIT_REGEX = \"\\/claims\\/P([0-9]*)$\"\n",
    "PROPERTY_STATEMENTS_EDIT_REGEX = \"^(?!.*(\\/hash|[0-9]\\/id|\\/references|\\/qualifiers))\\/claims(\\/P([0-9]*))?.*$\"\n",
    "PROPERTY_REFERENCES_EDIT_REGEX = \"^(?!.*(\\/hash|[0-9]\\/id))\\/claims\\/P([0-9]*)\\/[0-9]*\\/references.*$\"\n",
    "PROPERTY_QUALIFIERS_EDIT_REGEX = \"^(?!.*(\\/hash|[0-9]\\/id))\\/claims\\/P([0-9]*)\\/[0-9]*\\/qualifiers.*$\"\n",
    "PROPERTY_ANY_EDIT_REGEX = \"^(?!.*(\\/hash|[0-9]\\/id))\\/claims(\\/P([0-9]*))?.*$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5eeaa03b-016a-4c04-b419-17dcbde6e5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ops_of_entity(collection, entity_id, path_regex, match_prop=True):\n",
    "    pipeline = [\n",
    "        {\"$match\": {\"entity_id\": entity_id}},\n",
    "        {\"$project\": {\"entity_diff\": 1, \"entity_id\": 1, \"id\": 1, \"timestamp\": 1}},\n",
    "        {\"$sort\": {\"borough\": 1}},\n",
    "        {\"$unwind\": \"$entity_diff\"},\n",
    "        {\"$match\": {\"entity_diff.path\": {\"$regex\": path_regex}}}\n",
    "    ]\n",
    "    \n",
    "    if match_prop:\n",
    "        pipeline.append({\"$addFields\": {\"prop\": {\"$regexFind\": {\"input\": \"$entity_diff.path\", \"regex\": \"P([0-9]*)\"}}}})\n",
    "\n",
    "    return list(collection.aggregate(pipeline))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea20dbf1-7b23-44ae-b049-da4922ef06bd",
   "metadata": {},
   "source": [
    "## Building the static entity rdf dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "e037f632-55ab-415c-ab35-891054c22c73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jsonpatch\n",
    "import pdb\n",
    "\n",
    "def rebuild_entity_statements(qid, revisions_percentage=0.8):\n",
    "    entity_ops = get_ops_of_entity(wd_revisions, qid, PROPERTY_STATEMENTS_EDIT_REGEX, match_prop=False)\n",
    "    if len(entity_ops) < 10:\n",
    "        return None\n",
    "    \n",
    "    split_idx = round(len(entity_ops) * revisions_percentage)\n",
    "    \n",
    "    all_diffs = [op['entity_diff'] for op in entity_ops[:split_idx]]\n",
    "    for diff in all_diffs:\n",
    "        if diff['path'] == '/claims' and diff['op'] == 'add' and diff['value'] == []:\n",
    "            diff['value'] = {}\n",
    "    patch = jsonpatch.JsonPatch(all_diffs)\n",
    "    return patch.apply({\"claims\": {}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "7557d57f-be2c-44e3-8b20-ffc5380df844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Namespace, Graph, URIRef, Literal, BNode\n",
    "from rdflib.namespace import FOAF, RDF, RDFS\n",
    "\n",
    "geo = Namespace(\"http://www.opengis.net/ont/geosparql#\")\n",
    "wd = Namespace(\"http://www.wikidata.org/entity/\")\n",
    "wdt = Namespace(\"http://www.wikidata.org/prop/direct/\")\n",
    "wdno = Namespace(\"http://www.wikidata.org/prop/novalue/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "818f7e3c-732f-404f-86b5-edf7b9425d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_wikibase_entityid(val):\n",
    "    if val['entity-type'] == 'item':\n",
    "        return wd[f\"Q{val['numeric-id']}\"]\n",
    "    elif val['entity-type'] == 'property':\n",
    "        return wdt[f\"P{val['numeric-id']}\"]\n",
    "    else:\n",
    "        return Literal(val)\n",
    "\n",
    "datavalue_to_rdf = {\n",
    "    'string': lambda val: Literal(val),\n",
    "    'wikibase-entityid': lambda val: parse_wikibase_entityid(val),\n",
    "    'globecoordinate': lambda val: Literal(f\"{val['latitude']},{val['longitude']}\", datatype=geo.wktLiteral),\n",
    "    'commonsMedia': lambda val: Literal(val),\n",
    "    'url': lambda val: Literal(val),\n",
    "    'external-id': lambda val: Literal(val),\n",
    "    'monolingualtext': lambda val: Literal(val['text'], lang=val['language']),\n",
    "    'quantity': lambda val: Literal(float(val['amount'])),\n",
    "    'time': lambda val: Literal(val['time'])\n",
    "}\n",
    "\n",
    "def snak_json_to_simple_object(snak_json, prop_id):\n",
    "    snak_type = snak_json['snaktype']\n",
    "    if snak_type == 'novalue':\n",
    "        return wdno[prop_id]\n",
    "    elif snak_type == 'somevalue':\n",
    "        return BNode()\n",
    "    else:\n",
    "        # snak has a value\n",
    "        if 'datavalue' in snak_json:\n",
    "            snak_datatype = snak_json['datavalue']['type']\n",
    "            snak_datavalue = snak_json['datavalue']['value']\n",
    "\n",
    "            return datavalue_to_rdf[snak_datatype](snak_datavalue)\n",
    "        else:\n",
    "            return BNode()\n",
    "\n",
    "def entity_json_to_rdf(entity_qid, entity_json):\n",
    "    graph = Graph()\n",
    "    graph.add((wd[entity_qid], RDF.type, wd.entity))\n",
    "    \n",
    "    if 'claims' in entity_json:\n",
    "        for prop_id, claims in entity_json['claims'].items():\n",
    "            for claim in claims:\n",
    "                snak_obj = snak_json_to_simple_object(claim['mainsnak'], prop_id)\n",
    "                graph.add((wd[entity_qid], wdt[prop_id], snak_obj))\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "6f54b896-c5fd-44bc-91a4-984dcce107ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1089219"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_entities = 0\n",
    "for kg_class in top_classes:\n",
    "    num_entities += max(2000, round(len(kg_class.instances) * 0.1))\n",
    "\n",
    "num_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "ce57e081-7c31-454c-8f7b-42aa47447d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_static_rdf_dataset(train_rev_split=0.8, num_entities_per_class=lambda num_instances: max(2000, round(num_instances * 0.1))):\n",
    "    train_graph = Graph()\n",
    "    test_graph = Graph()\n",
    "    graphs = [train_graph, test_graph]\n",
    "\n",
    "    for g in graphs:\n",
    "        g.bind(\"geo\", geo)\n",
    "        g.bind(\"wd\", wd)\n",
    "        g.bind(\"wdt\", wdt)\n",
    "        g.bind(\"wdno\", wdno)\n",
    "\n",
    "    rng = random.Random(RANDOM_SEED)\n",
    "    for kg_class in tqdm(top_classes):\n",
    "        for g in graphs:\n",
    "            g.add((wd[kg_class.qid], RDF.type, wd.kg_class))\n",
    "        \n",
    "        rng.shuffle(kg_class.instances)\n",
    "        end_idx = num_entities_per_class(len(kg_class.instances))\n",
    "        for entity in tqdm(kg_class.instances[:end_idx]):\n",
    "            entity_qid = entity.qid\n",
    "            rebuilt_train_entity = rebuild_entity_statements(entity_qid, revisions_percentage=train_rev_split)\n",
    "            rebuilt_complete_entity = rebuild_entity_statements(entity_qid, revisions_percentage=1.0)\n",
    "            if rebuilt_train_entity is not None:\n",
    "                train_entity_graph = entity_json_to_rdf(entity_qid, rebuilt_train_entity)\n",
    "                complete_entity_graph = entity_json_to_rdf(entity_qid, rebuilt_complete_entity)\n",
    "                \n",
    "                train_graph += train_entity_graph\n",
    "                test_graph += (complete_entity_graph - train_graph)\n",
    "\n",
    "    return train_graph, test_graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "04a4f755-0a50-4e3c-8fae-6a7bfb89bd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5baf5825062c4e0d8f266edb2594f013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6036c7e884024f7688328aef5c2a48ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [313]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_graph, complete_graph \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_static_rdf_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_rev_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_entities_per_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnum_instances\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m train_graph\u001b[38;5;241m.\u001b[39mserialize(destination\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUTPUT_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_graph_sample.ttl\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      4\u001b[0m complete_graph\u001b[38;5;241m.\u001b[39mserialize(destination\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUTPUT_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcomplete_graph_sample.ttl\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Input \u001b[0;32mIn [312]\u001b[0m, in \u001b[0;36mbuild_static_rdf_dataset\u001b[0;34m(train_rev_split, num_entities_per_class)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entity \u001b[38;5;129;01min\u001b[39;00m tqdm(kg_class\u001b[38;5;241m.\u001b[39minstances[:end_idx]):\n\u001b[1;32m     20\u001b[0m     entity_qid \u001b[38;5;241m=\u001b[39m entity\u001b[38;5;241m.\u001b[39mqid\n\u001b[0;32m---> 21\u001b[0m     rebuilt_train_entity \u001b[38;5;241m=\u001b[39m \u001b[43mrebuild_entity_statements\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentity_qid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevisions_percentage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_rev_split\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     rebuilt_complete_entity \u001b[38;5;241m=\u001b[39m rebuild_entity_statements(entity_qid, revisions_percentage\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rebuilt_train_entity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Input \u001b[0;32mIn [238]\u001b[0m, in \u001b[0;36mrebuild_entity_statements\u001b[0;34m(qid, revisions_percentage)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrebuild_entity_statements\u001b[39m(qid, revisions_percentage\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     entity_ops \u001b[38;5;241m=\u001b[39m \u001b[43mget_ops_of_entity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwd_revisions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPROPERTY_STATEMENTS_EDIT_REGEX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatch_prop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(entity_ops) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m10\u001b[39m:\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Input \u001b[0;32mIn [71]\u001b[0m, in \u001b[0;36mget_ops_of_entity\u001b[0;34m(collection, entity_id, path_regex, match_prop)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m match_prop:\n\u001b[1;32m     11\u001b[0m     pipeline\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$addFields\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprop\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$regexFind\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$entity_diff.path\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregex\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP([0-9]*)\u001b[39m\u001b[38;5;124m\"\u001b[39m}}}})\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[43mcollection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maggregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pymongo/collection.py:1971\u001b[0m, in \u001b[0;36mCollection.aggregate\u001b[0;34m(self, pipeline, session, **kwargs)\u001b[0m\n\u001b[1;32m   1901\u001b[0m \u001b[38;5;124;03m\"\"\"Perform an aggregation using the aggregation framework on this\u001b[39;00m\n\u001b[1;32m   1902\u001b[0m \u001b[38;5;124;03mcollection.\u001b[39;00m\n\u001b[1;32m   1903\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1968\u001b[0m \u001b[38;5;124;03m    https://docs.mongodb.com/manual/reference/command/aggregate\u001b[39;00m\n\u001b[1;32m   1969\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1970\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__database\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39m_tmp_session(session, close\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m s:\n\u001b[0;32m-> 1971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_aggregate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_CollectionAggregationCommand\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1972\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1973\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mCommandCursor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1974\u001b[0m \u001b[43m                           \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1975\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mexplicit_session\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1976\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pymongo/collection.py:1896\u001b[0m, in \u001b[0;36mCollection._aggregate\u001b[0;34m(self, aggregation_command, pipeline, cursor_class, session, explicit_session, **kwargs)\u001b[0m\n\u001b[1;32m   1891\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_aggregate\u001b[39m(\u001b[38;5;28mself\u001b[39m, aggregation_command, pipeline, cursor_class, session,\n\u001b[1;32m   1892\u001b[0m                explicit_session, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1893\u001b[0m     cmd \u001b[38;5;241m=\u001b[39m aggregation_command(\n\u001b[1;32m   1894\u001b[0m         \u001b[38;5;28mself\u001b[39m, cursor_class, pipeline, kwargs, explicit_session,\n\u001b[1;32m   1895\u001b[0m         user_fields\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcursor\u001b[39m\u001b[38;5;124m'\u001b[39m: {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirstBatch\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m}})\n\u001b[0;32m-> 1896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__database\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retryable_read\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1897\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcmd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_read_preference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1898\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretryable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcmd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_performs_write\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pymongo/mongo_client.py:1313\u001b[0m, in \u001b[0;36mMongoClient._retryable_read\u001b[0;34m(self, func, read_pref, session, address, retryable)\u001b[0m\n\u001b[1;32m   1309\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m retrying \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m retryable:\n\u001b[1;32m   1310\u001b[0m             \u001b[38;5;66;03m# A retry is not possible because this server does\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m             \u001b[38;5;66;03m# not support retryable reads, raise the last error.\u001b[39;00m\n\u001b[1;32m   1312\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m last_error\n\u001b[0;32m-> 1313\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mserver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msock_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecondary_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ServerSelectionTimeoutError:\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retrying:\n\u001b[1;32m   1316\u001b[0m         \u001b[38;5;66;03m# The application may think the write was never attempted\u001b[39;00m\n\u001b[1;32m   1317\u001b[0m         \u001b[38;5;66;03m# if we raise ServerSelectionTimeoutError on the retry\u001b[39;00m\n\u001b[1;32m   1318\u001b[0m         \u001b[38;5;66;03m# attempt. Raise the original exception instead.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pymongo/aggregation.py:127\u001b[0m, in \u001b[0;36m_AggregationCommand.get_cursor\u001b[0;34m(self, session, server, sock_info, secondary_ok)\u001b[0m\n\u001b[1;32m    124\u001b[0m     write_concern \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Run command.\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43msock_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommand\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_database\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43msecondary_ok\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_read_preference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_target\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodec_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_write_concern_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mread_concern\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mread_concern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_concern\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_concern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_database\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_fields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_user_fields\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_result(result, session, server, sock_info, secondary_ok)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# Extract cursor from result or mock/fake one if necessary.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pymongo/pool.py:735\u001b[0m, in \u001b[0;36mSocketInfo.command\u001b[0;34m(self, dbname, spec, secondary_ok, read_preference, codec_options, check, allowable_errors, check_keys, read_concern, write_concern, parse_write_concern_error, collation, session, client, retryable_write, publish_events, user_fields, exhaust_allowed)\u001b[0m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;66;03m# Catch socket.error, KeyboardInterrupt, etc. and close ourselves.\u001b[39;00m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m--> 735\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_connection_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pymongo/pool.py:719\u001b[0m, in \u001b[0;36mSocketInfo.command\u001b[0;34m(self, dbname, spec, secondary_ok, read_preference, codec_options, check, allowable_errors, check_keys, read_concern, write_concern, parse_write_concern_error, collation, session, client, retryable_write, publish_events, user_fields, exhaust_allowed)\u001b[0m\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_if_not_writable(unacknowledged)\n\u001b[1;32m    718\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 719\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcommand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdbname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msecondary_ok\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m                   \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_mongos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_preference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodec_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m                   \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallowable_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m                   \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlisteners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m                   \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_bson_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_concern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mparse_write_concern_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_write_concern_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mcollation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mcompression_ctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m                   \u001b[49m\u001b[43muse_op_msg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mop_msg_enabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m                   \u001b[49m\u001b[43munacknowledged\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munacknowledged\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m                   \u001b[49m\u001b[43muser_fields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mexhaust_allowed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexhaust_allowed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (OperationFailure, NotPrimaryError):\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pymongo/network.py:149\u001b[0m, in \u001b[0;36mcommand\u001b[0;34m(sock_info, dbname, spec, secondary_ok, is_mongos, read_preference, codec_options, session, client, check, allowable_errors, address, check_keys, listeners, max_bson_size, read_concern, parse_write_concern_error, collation, compression_ctx, use_op_msg, unacknowledged, user_fields, exhaust_allowed)\u001b[0m\n\u001b[1;32m    147\u001b[0m     response_doc \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mok\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m}\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 149\u001b[0m     reply \u001b[38;5;241m=\u001b[39m \u001b[43mreceive_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m     sock_info\u001b[38;5;241m.\u001b[39mmore_to_come \u001b[38;5;241m=\u001b[39m reply\u001b[38;5;241m.\u001b[39mmore_to_come\n\u001b[1;32m    151\u001b[0m     unpacked_docs \u001b[38;5;241m=\u001b[39m reply\u001b[38;5;241m.\u001b[39munpack_response(\n\u001b[1;32m    152\u001b[0m         codec_options\u001b[38;5;241m=\u001b[39mcodec_options, user_fields\u001b[38;5;241m=\u001b[39muser_fields)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pymongo/network.py:197\u001b[0m, in \u001b[0;36mreceive_message\u001b[0;34m(sock_info, request_id, max_message_size)\u001b[0m\n\u001b[1;32m    194\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# Ignore the response's request id.\u001b[39;00m\n\u001b[1;32m    196\u001b[0m length, _, response_to, op_code \u001b[38;5;241m=\u001b[39m _UNPACK_HEADER(\n\u001b[0;32m--> 197\u001b[0m     \u001b[43m_receive_data_on_socket\u001b[49m\u001b[43m(\u001b[49m\u001b[43msock_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeadline\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# No request_id for exhaust cursor \"getMore\".\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/pymongo/network.py:262\u001b[0m, in \u001b[0;36m_receive_data_on_socket\u001b[0;34m(sock_info, length, deadline)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m     wait_for_read(sock_info, deadline)\n\u001b[0;32m--> 262\u001b[0m     chunk_length \u001b[38;5;241m=\u001b[39m \u001b[43msock_info\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmv\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbytes_read\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIOError\u001b[39;00m, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _errno_from_exception(exc) \u001b[38;5;241m==\u001b[39m errno\u001b[38;5;241m.\u001b[39mEINTR:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_graph, test_graph = build_static_rdf_dataset(train_rev_split=0.7, num_entities_per_class=lambda num_instances: 5000)\n",
    "\n",
    "train_graph.serialize(destination=os.path.join(OUTPUT_DIR, 'train_graph_sample.ttl'))\n",
    "test_graph.serialize(destination=os.path.join(OUTPUT_DIR, 'test_graph_sample.ttl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d9b7f1-6ed1-4bdb-82e4-8b4d374e1b36",
   "metadata": {},
   "source": [
    "Quick check to see how many new 'instance of' values have appeared:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448025ff-264b-4167-83ed-8324b0bd1f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_graph = complete_graph - train_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97d361f-5842-48d0-9c25-0fc794da9d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_new_instance_of = 0\n",
    "\n",
    "for t in diff_graph.triples((None, wdt.P31, None)):\n",
    "    num_new_instance_of += 1\n",
    "num_new_instance_of"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3d12c9-d90c-4b62-8b5d-f8728dc3bb93",
   "metadata": {},
   "source": [
    "**TODO: maybe we should remove from the dataset entities without new instance of data in the test dataset**\n",
    "\n",
    "**Option 2: we keep those entities but don't take them into account when evaluating the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4225b257-e29d-41af-b9fd-7d3562ef1038",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Building the dynamic entity rdf dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2a84dd-c2d7-46fd-b6e9-894d4a31bd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_rev_data_to_rdf():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3507ecdb-1368-418a-a3e8-881aa9151b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dynamic_rdf_dataset():\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1170665-4ba5-4cb7-b214-c515557b74e4",
   "metadata": {},
   "source": [
    "## Testing zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032d63ee-b5aa-43ca-9cba-e39b0f74a317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4572ffe4-7dc5-41e5-8bbd-f1f62314e824",
   "metadata": {},
   "source": [
    "Clasificador:\n",
    "* Entrada:\n",
    "    * 'Representación' de la entidad (e.g. embedding con info temporal o sin ella dependiendo del clasificador).\n",
    "    * 'Representación' de la propiedad.\n",
    "    * Tipo de operación nueva.\n",
    "    * Path de la operación.\n",
    "    * Valor nuevo de la operación.\n",
    "* Salida:\n",
    "    * Se acepta o no (0/1).\n",
    "    * Dependiendo del clasificador, umbral de confianza.\n",
    "    \n",
    "Podríamos tener los siguientes clasificadores:\n",
    "* a) Baseline: utilizando solo info de la entidad actual (sin edit history).\n",
    "* b) El mismo que antes, pero utilizando información del historial de ediciones. Este se podría dividir en varios en función de la información utilizada (p.ej. uno que analiza el historial global vs otro que utiliza info de cada decil).\n",
    "* c) Mirando si esa operación fue eliminada antes (lo rechaza directamente), y si no lo pasa a uno de los clasificadores anteriores.\n",
    "\n",
    "Podríamos probar las siguientes cosas:\n",
    "* Clasificadores de antes con la info hasta noviembre (lo que tenemos indexado), y ver el rendimiento en las operaciones que pasaron desde entonces hasta 1 de marzo p.ej.\n",
    "* Clasificadores con historial de ediciones. Vamos viendo como mejora el rendimiento cuanta mas info tenemos (p.ej. tiramos solo del 20% de ediciones para predecir lo siguiente, luego del 40%, del 60%...).\n",
    "* Diferencia en los resultados entre clases (en cuales hay mas precision, menos...). Esto lo podemos encadenar con el classrank, o las métricas de conflicto que saquemos para ver la relación existente. Esto puede ser útil para saber qué clases tienen mayor potencial de aplicar estas técnicas y cuáles no."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
