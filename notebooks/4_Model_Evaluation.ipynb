{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d46ee87-7b0d-4461-9e7f-bad7a37e78d3",
   "metadata": {},
   "source": [
    "# 4.- Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f53d6d-68c8-4e72-80fe-abae3d2034b7",
   "metadata": {},
   "source": [
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7b509f2-e01f-4be3-9388-103f32a7e1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b776d1b-773f-470c-b2e1-31fe46d5a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Namespace, Graph, URIRef, Literal, BNode\n",
    "from rdflib.namespace import FOAF, RDF, RDFS\n",
    "\n",
    "geo = Namespace(\"http://www.opengis.net/ont/geosparql#\")\n",
    "uo = Namespace(\"https://purl.org/uniovi/wd-edit-history#\")\n",
    "wd = Namespace(\"http://www.wikidata.org/entity/\")\n",
    "wdt = Namespace(\"http://www.wikidata.org/prop/direct/\")\n",
    "wdno = Namespace(\"http://www.wikidata.org/prop/novalue/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b50a3636-05f5-4cbd-9cb3-9a9a1bc7dff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "REFINEMENT_SYSTEMS_OUTPUT_DIR = os.path.join('output', '3_edit_history_systems')\n",
    "OUTPUT_DIR = os.path.join('output', '4_model_evaluation')\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "WIKIDATA_BASE = \"https://www.wikidata.org/w/api.php\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15b536df-b2b2-4065-b266-3352c7eacf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(filename):\n",
    "    with open(os.path.join(OUTPUT_DIR, f\"{filename}.pkl\"), 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def save_df(df, filename):\n",
    "    with open(os.path.join(OUTPUT_DIR, f\"{filename}.pkl\"), 'wb') as f:\n",
    "        pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bc4dc34-ec7b-454d-8c71-f7169705b4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_graph_static = Graph().parse(os.path.join(REFINEMENT_SYSTEMS_OUTPUT_DIR, 'complete_train_graph_static.ttl'), format='ttl')\n",
    "test_graph_static = Graph().parse(os.path.join(REFINEMENT_SYSTEMS_OUTPUT_DIR, 'test_graph_sample_static.ttl'), format='ttl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15ee262a-0276-4e30-96b6-18a90649293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from pykeen.triples import TriplesFactory\n",
    "\n",
    "known_entities = set()\n",
    "known_relations = set()\n",
    "\n",
    "def entity_hashed_id(entity_uri):\n",
    "    return hash(entity_uri.split('/')[-1])\n",
    "\n",
    "def build_triples_factory_train(graph):\n",
    "    triples = []\n",
    "    for t in graph:\n",
    "        if not t[2].startswith(str(wdt)) and not t[2].startswith(str(wd)):\n",
    "            continue\n",
    "            \n",
    "        triple_str = [str(t[0]), str(t[1]), str(t[2])]\n",
    "        if triple_str[0] not in known_entities:\n",
    "            known_entities.add(triple_str[0])\n",
    "        \n",
    "        if triple_str[1] not in known_relations:\n",
    "            known_relations.add(triple_str[1])\n",
    "        \n",
    "        if triple_str[2] not in known_entities:\n",
    "            known_entities.add(triple_str[2])\n",
    "\n",
    "        triples.append(triple_str)\n",
    "    return TriplesFactory.from_labeled_triples(np.array(triples))\n",
    "\n",
    "def build_triples_factory_test(graph, train_tf):\n",
    "    triples = []\n",
    "    for t in graph:\n",
    "        if not t[2].startswith(str(wdt)) and not t[2].startswith(str(wd)):\n",
    "            continue\n",
    "        \n",
    "        triple_str = [str(t[0]), str(t[1]), str(t[2])]\n",
    "        if triple_str[0] not in known_entities or triple_str[1] not in known_relations or triple_str[2] not in known_entities:\n",
    "            continue\n",
    "        triples.append(triple_str)\n",
    "\n",
    "    return TriplesFactory.from_labeled_triples(np.array(triples), entity_to_id=train_tf.entity_to_id, relation_to_id=train_tf.relation_to_id)\n",
    "\n",
    "tf_train = build_triples_factory_train(train_graph_static)\n",
    "tf_test = build_triples_factory_test(test_graph_static, tf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9b37ca-997c-43eb-a0af-1525df7cf3f0",
   "metadata": {},
   "source": [
    "## Defining the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65558ed6-6e58-4d23-8bf4-b126c493940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_at_n(ranks, n):\n",
    "    return sum([1 if r <= n else 0 for r in ranks]) / len(ranks)\n",
    "\n",
    "def mr_score(ranks):\n",
    "    mr_score = 0\n",
    "    for r in ranks:\n",
    "        mr_score += r\n",
    "    return mr_score / len(ranks)\n",
    "\n",
    "def mrr_score(ranks):\n",
    "    mr_score = 0\n",
    "    for r in ranks:\n",
    "        mr_score += 1 / r\n",
    "    return mr_score / len(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145a262f-4331-4536-84fb-5aef5daaad0f",
   "metadata": {},
   "source": [
    "## Testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3bb659d-4020-49bf-bc0e-03535324a9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "\n",
    "def get_testing_entities(test_graph_static, prop = None):\n",
    "    # get all entities that have a new P31 value in the test data\n",
    "    entities_test = set()\n",
    "    for t in test_graph_static.triples((None, prop, None)):\n",
    "        entities_test.add((str(t[0]), str(t[1]), str(t[2])))\n",
    "    return entities_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ab954c3-2f1e-441b-a04b-80477aa70a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_supervised(model, entities_test, entity_2_embeddings, all_classes):\n",
    "    ranks = []\n",
    "    misses = 0\n",
    "    _all = 0\n",
    "    \n",
    "    # generic model useful for any property, embeddings = subj + prop + obj\n",
    "    embedding_size = len(list(entity_2_embeddings.values())[0])\n",
    "    X = np.zeros((len(all_classes), embedding_size * 3))\n",
    "    for i, kg_class in enumerate(all_classes):\n",
    "        X[i, 2*embedding_size:] = entity_2_embeddings[kg_class]\n",
    "        \n",
    "    \n",
    "    for entity, prop, true_class in tqdm(entities_test):\n",
    "        _all += 1\n",
    "\n",
    "        X[:, :embedding_size] = entity_2_embeddings[entity]\n",
    "        X[:, embedding_size:2*embedding_size] = entity_2_embeddings[prop]\n",
    "\n",
    "        pred = model.predict_proba(X)\n",
    "        entity_results = [(kg_class, pred[idx][1]) for idx, kg_class in enumerate(all_classes)]\n",
    "        entity_results.sort(key=lambda item: item[1], reverse=True)\n",
    "        sorted_predictions = [e[0] for e in entity_results]\n",
    "        if true_class not in sorted_predictions:\n",
    "            misses += 1\n",
    "            continue\n",
    "        idx = sorted_predictions.index(true_class)\n",
    "        ranks.append(idx + 1)\n",
    "    return ranks, misses, _all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e3459047-af3b-4aef-9a3a-2809a6a8b928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykeen.models import predict\n",
    "\n",
    "def test_unsupervised(model, entities_test, triples_factory, train_graph_static):\n",
    "    ranks = []\n",
    "    misses = 0\n",
    "    _all = 0\n",
    "    for entity, prop, true_class in tqdm(entities_test):\n",
    "        _all += 1\n",
    "        if entity not in triples_factory.entity_to_id or prop not in triples_factory.relation_to_id:\n",
    "            misses += 1\n",
    "            continue\n",
    "        \n",
    "        pred_df = predict.get_tail_prediction_df(model, entity, str(prop), triples_factory=triples_factory, add_novelties=False)\n",
    "        pred_rank = 1\n",
    "        for row in pred_df.itertuples():\n",
    "            if row[2] == true_class:\n",
    "                ranks.append(pred_rank)\n",
    "                break\n",
    "            else:\n",
    "                # manual filtering of known triples\n",
    "                if (URIRef(entity), URIRef(prop), URIRef(row[2])) not in train_graph_static:\n",
    "                    pred_rank += 1\n",
    "    return ranks, misses, _all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf71ef3-8cee-4ce1-8777-5b1e3a306873",
   "metadata": {},
   "source": [
    "## Evaluating the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9910885-9b6d-42be-9c0e-17bdcdb09f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "props_to_evaluate = [wdt.P31]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce7a9f1-5147-4ccf-94f6-62475885645d",
   "metadata": {},
   "source": [
    "### Unsupervised models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc69e3be-c174-4bba-a8cd-1bb7c33f32d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with open('model.pkl', 'rb') as f:\n",
    "    model = torch.load(f, map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15a687c2-5778-49c1-a751-55c72a908f34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e668956204a74c7289535764fc1cbd80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4186 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MR score: 10819.282152230971\n",
      "MRR score: 0.1774430164555107\n",
      "hits@1: 0.08582677165354331\n",
      "hits@5: 0.3\n",
      "hits@10: 0.38188976377952755\n",
      "9\n",
      "4186\n"
     ]
    }
   ],
   "source": [
    "ranks, misses, _all = test_system_unsupervised(model, test_entities, tf_train, train_graph_static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "97d0b8e7-d2dd-424b-b273-d4b4b4322362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1319, 190, 3846, 5, 13]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranks[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883ddcbd-defa-4320-8e10-d677e9c13283",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'model': [],\n",
    "    'sampler': [],\n",
    "    'prop': [],\n",
    "    'MR': [],\n",
    "    'MRR': [],\n",
    "    'hits@1': [],\n",
    "    'hits@5': [],\n",
    "    'hits@10': []\n",
    "}\n",
    "\n",
    "TRAINED_MODELS_DIR = os.path.join(REFINEMENT_SYSTEMS_OUTPUT_DIR, 'models')\n",
    "model_directories = [f for f in os.listdir(TRAINED_MODELS_DIR) if os.path.isdir(os.path.join(TRAINED_MODELS_DIR, f))]\n",
    "\n",
    "for prop in props_to_evaluate:\n",
    "    logger.info(str(prop))\n",
    "    test_entities = get_testing_entities(test_graph_static, prop)\n",
    "    for model_dir in model_directories:\n",
    "        logger.info(model_dir)\n",
    "        samplers_directories = [f for f in os.listdir(os.path.join(TRAINED_MODELS_DIR, model_dir)) if os.path.isdir(os.path.join(TRAINED_MODELS_DIR, model_dir, f))]\n",
    "        for sampler in samplers_directories:\n",
    "            logger.info(sampler)\n",
    "                path = os.path.join(MODEL_DIR, model_dir, sampler)\n",
    "                with open(os.path.join(path, 'model.pkl'), 'rb') as f:\n",
    "                    model = torch.load(f, map_location=torch.device('cpu'))\n",
    "                ranks, misses, _all = test_system_unsupervised(model, test_entities, tf_train, train_graph_static)\n",
    "                data['model'].append(model_dir)\n",
    "                data['sampler'].append(sampler)\n",
    "                data['prop'].append(str(prop))\n",
    "                data['MR'].append(mr_score(ranks))\n",
    "                data['MRR'].append(mrr_score(ranks))\n",
    "                data['hits@1'].append(hits_at_n(ranks, 1))\n",
    "                data['hits@5'].append(hits_at_n(ranks, 5))\n",
    "                data['hits@10'].append(hits_at_n(ranks, 10))\n",
    "    logger.info(\"\\n\\n\")\n",
    "\n",
    "evaluation_unsupervised_df = pd.DataFrame(data)\n",
    "save_df(evaluation_unsupervised_df, 'evaluation_unsupervised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844ecf06-2cc8-4c97-8632-bde41bcce5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_unsupervised_df = load_df('evaluation_unsupervised')\n",
    "evaluation_unsupervised_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b61d84-9973-4f30-836f-a58bc46fd7f0",
   "metadata": {},
   "source": [
    "### Supervised model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dae1ad-597e-4af2-9d99-64a56e4e752c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(TRAINED_MODELS_DIR, 'supervised', 'model.pkl'), 'rb') as f:\n",
    "    supervised_model = pickle.load(f)\n",
    "\n",
    "with open(os.path.join(TRAINED_MODELS_DIR, 'supervised', 'embeddings.pkl'), 'rb') as f: \n",
    "    entity_2_embeddings = pickle.dump(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaae6b7f-6a68-40a1-9490-7b9d295f5caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'prop': [],\n",
    "    'MR': [],\n",
    "    'MRR': [],\n",
    "    'hits@1': [],\n",
    "    'hits@5': [],\n",
    "    'hits@10': []\n",
    "}\n",
    "\n",
    "possible_entities = list(set([str(s) for s in train_graph_static.subjects(None, None) if isinstance(s, URIRef)] + \\\n",
    "                             [str(s) for s in train_graph_static.objects(None, None) if isinstance(s, URIRef)]))\n",
    "\n",
    "for prop in props_to_evaluate:\n",
    "    logger.info(str(prop))\n",
    "    entities_test = get_testing_entities(test_graph_static, prop)\n",
    "    ranks, misses, _all = test_supervised(supervised_model, entities_test, entity_2_embeddings, possible_entities)\n",
    "    data['prop'].append(str(prop))\n",
    "    data['MR'].append(mr_score(ranks))\n",
    "    data['MRR'].append(mrr_score(ranks))\n",
    "    data['hits@1'].append(hits_at_n(ranks, 1))\n",
    "    data['hits@5'].append(hits_at_n(ranks, 5))\n",
    "    data['hits@10'].append(hits_at_n(ranks, 10))\n",
    "    \n",
    "evaluation_supervised_df = pd.DataFrame(data)\n",
    "save_df(evaluation_supervised_df, 'evaluation_supervised')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbc6f49-5434-4758-b8df-df95e87e8666",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_supervised_df = load_df('evaluation_supervised')\n",
    "evaluation_supervised_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
