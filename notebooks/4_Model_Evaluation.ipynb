{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d46ee87-7b0d-4461-9e7f-bad7a37e78d3",
   "metadata": {},
   "source": [
    "# 4.- Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f53d6d-68c8-4e72-80fe-abae3d2034b7",
   "metadata": {},
   "source": [
    "## Notebook setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50a3636-05f5-4cbd-9cb3-9a9a1bc7dff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c9b37ca-997c-43eb-a0af-1525df7cf3f0",
   "metadata": {},
   "source": [
    "## Defining the evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65558ed6-6e58-4d23-8bf4-b126c493940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_at_n(ranks, n):\n",
    "    return sum([1 if r <= n else 0 for r in ranks]) / len(ranks)\n",
    "\n",
    "def mr_score(ranks):\n",
    "    mr_score = 0\n",
    "    for r in ranks:\n",
    "        mr_score += r\n",
    "    return mr_score / len(ranks)\n",
    "\n",
    "def mrr_score(ranks):\n",
    "    mr_score = 0\n",
    "    for r in ranks:\n",
    "        mr_score += 1 / r\n",
    "    return mr_score / len(ranks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145a262f-4331-4536-84fb-5aef5daaad0f",
   "metadata": {},
   "source": [
    "## Testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bb659d-4020-49bf-bc0e-03535324a9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "\n",
    "def get_testing_entities(test_graph_static, prop = None):\n",
    "    # get all entities that have a new P31 value in the test data\n",
    "    entities_test = set()\n",
    "    for t in test_graph_static.triples((None, prop, None)):\n",
    "        entities_test.add((str(t[0]), str(t[1]), str(t[2])))\n",
    "    return entities_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab954c3-2f1e-441b-a04b-80477aa70a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_system_supervised(model, entities_test, entity_2_embeddings, all_classes, specific_prop=False):\n",
    "    ranks = []\n",
    "    misses = 0\n",
    "    _all = 0\n",
    "    \n",
    "    # set up pred array\n",
    "    if specific_prop:\n",
    "        # model trained for a specific property, embeddings = subj + obj\n",
    "        embedding_size = len(list(entity_2_embeddings.values())[0])\n",
    "        X = np.zeros((len(all_classes), embedding_size * 2))\n",
    "        for i, kg_class in enumerate(all_classes):\n",
    "            X[i, embedding_size:] = entity_2_embeddings[kg_class]\n",
    "    else:\n",
    "        # generic model useful for any property, embeddings = subj + prop + obj\n",
    "        embedding_size = len(list(entity_2_embeddings.values())[0])\n",
    "        X = np.zeros((len(all_classes), embedding_size * 3))\n",
    "        for i, kg_class in enumerate(all_classes):\n",
    "            X[i, 2*embedding_size:] = entity_2_embeddings[kg_class]\n",
    "        \n",
    "    \n",
    "    for entity, prop, true_class in tqdm(entities_test):\n",
    "        _all += 1\n",
    "        #print(f\"Entity: {entity}\")\n",
    "        #print(\"-\" * 25)\n",
    "            \n",
    "        if specific_prop:\n",
    "            X[:, :embedding_size] = entity_2_embeddings[entity]\n",
    "        else:\n",
    "            X[:, :embedding_size] = entity_2_embeddings[entity]\n",
    "            X[:, embedding_size:2*embedding_size] = entity_2_embeddings[prop]\n",
    "\n",
    "        pred = model.predict_proba(X)\n",
    "        entity_results = [(kg_class, pred[idx][1]) for idx, kg_class in enumerate(all_classes)]\n",
    "        entity_results.sort(key=lambda item: item[1], reverse=True)\n",
    "        sorted_predictions = [e[0] for e in entity_results]\n",
    "        if true_class not in sorted_predictions:\n",
    "            misses += 1\n",
    "            continue\n",
    "        idx = sorted_predictions.index(true_class)\n",
    "        ranks.append(idx + 1)\n",
    "    print(f\"MR score: {mr_score(ranks)}\")\n",
    "    print(f\"MRR score: {mrr_score(ranks)}\")\n",
    "    print(f\"hits@1: {hits_at_n(ranks, 1)}\")\n",
    "    print(f\"hits@5: {hits_at_n(ranks, 5)}\")\n",
    "    print(f\"hits@10: {hits_at_n(ranks, 10)}\")\n",
    "    print(misses)\n",
    "    print(_all)\n",
    "    return ranks, misses, _all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3459047-af3b-4aef-9a3a-2809a6a8b928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_system_unsupervised(model, entities_test):\n",
    "    ranks = []\n",
    "    misses = 0\n",
    "    _all = 0\n",
    "    for entity, prop, true_class in tqdm(entities_test):\n",
    "        _all += 1\n",
    "        sorted_predictions = model.predict_tail(entity, prop)\n",
    "        if true_class not in sorted_predictions:\n",
    "            misses += 1\n",
    "            continue\n",
    "        idx = sorted_predictions.index(true_class)\n",
    "        ranks.append(idx + 1)\n",
    "    \n",
    "    print(f\"MR score: {mr_score(ranks)}\")\n",
    "    print(f\"MRR score: {mrr_score(ranks)}\")\n",
    "    print(f\"hits@1: {hits_at_n(ranks, 1)}\")\n",
    "    print(f\"hits@5: {hits_at_n(ranks, 5)}\")\n",
    "    print(f\"hits@10: {hits_at_n(ranks, 10)}\")\n",
    "    print(misses)\n",
    "    print(_all)\n",
    "    return ranks, misses, _all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf71ef3-8cee-4ce1-8777-5b1e3a306873",
   "metadata": {},
   "source": [
    "## Evaluating the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3da1a5-ce61-408a-913a-3ad0038ca4be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc69e3be-c174-4bba-a8cd-1bb7c33f32d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
